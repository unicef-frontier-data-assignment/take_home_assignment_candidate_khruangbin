{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the Unicef Data Engineer assignment\n",
    "\n",
    "### TASKS / TODO's and estimation (T-Shirt sizes)\n",
    "1. Download the Ookla data from Ooklaâ€™s S3 bucket - for the most recent 12 months (4 quarters) - Effort: M\n",
    "    1. Combine the quarterly data into a sensible average (Sum of # devices, Mean of upload / download speeds)\n",
    "2. Download the most recent Worldpop population data from HDX (Preferably using the HDX API) - Effort: M\n",
    "    1. Sum the total school age population (5-20yrs old) for each school polygon\n",
    "3. Merge this data with the existing school polygons geojson - Effort: S\n",
    "4. Write all this to a new geojson file that [Data Visualisation Specialist] can visualise - Effort: S\n",
    "\n",
    "\n",
    "#### Notes\n",
    "- First I want to explore the datasets that I need to use. I want to know what format they are in, how I can access them and what their sizes are. Therefore I immediately install the `geowrangler` library for Python and try to access the Ookla data.\n",
    "- I also download TIF files from the Humanitarian Data Exchange.\n",
    "- I also set up a virtual environment locally, so that I can easily manage the desired python version and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this Notebook\n",
    "\n",
    "### Python version used:\n",
    "- Python 3.9.10\n",
    "\n",
    "### Install python dependencies from `requirements.txt` file:\n",
    "```pip install -r requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the schools_by_district data\n",
    "import geopandas as gpd\n",
    "\n",
    "df_schools = gpd.read_file(\"schools_by_district.geojson\")\n",
    "df_schools.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Ookla datasets\n",
    "\n",
    "# importing ookla from geowrangler\n",
    "from geowrangler.datasets import ookla\n",
    "\n",
    "# checking which methods and properties this library contains\n",
    "print(dir(ookla))\n",
    "\n",
    "# listing the available dataset files\n",
    "ookla_datasets = ookla.list_ookla_files()\n",
    "print(ookla_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ookla data\n",
    "\n",
    "# We want to use dask for its batch processing capabilities for large data\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# we want to download the last 4 quarters, that we explored above\n",
    "ookla_quarter_params = [\n",
    "    dict(year=\"2021\", quarter=\"4\", directory=\"data\"),\n",
    "    dict(year=\"2022\", quarter=\"1\", directory=\"data\"),\n",
    "    dict(year=\"2022\", quarter=\"2\", directory=\"data\"),\n",
    "    dict(year=\"2022\", quarter=\"3\", directory=\"data\")\n",
    "]\n",
    "\n",
    "all_data_ookla = None\n",
    "for quarter_params in ookla_quarter_params:\n",
    "    data_ookla = ookla.download_ookla_file(type_=\"fixed\", **quarter_params)\n",
    "    data_ookla = dd.read_parquet(data_ookla, engine='pyarrow')\n",
    "    data_ookla = data_ookla.drop(columns=[\"avg_lat_ms\", \"tests\"])\n",
    "\n",
    "    if all_data_ookla is not None:\n",
    "        all_data_ookla = dd.concat([all_data_ookla, data_ookla])\n",
    "    else:\n",
    "        all_data_ookla = data_ookla\n",
    "\n",
    "all_data_ookla = all_data_ookla.groupby([\"quadkey\", \"tile\"]).agg({\"avg_d_kbps\": ['mean'], \"avg_u_kbps\": [\"mean\"], \"devices\": [\"sum\"]}).compute()\n",
    "all_data_ookla = all_data_ookla.droplevel(axis=1, level=1).reset_index()\n",
    "\n",
    "# Now I want to turn the \"all_data_ookla\" into a GeoPandas dataframe, using \"tile\" as the geometry.\n",
    "# Here I kept getting some strange TypeError, and I spent long time figuring out the issue\n",
    "# TypeError: Input must be valid geometry objects: POLYGON((-160.037841796875 70.6363054807905, -160.032348632812 70.6363054807905, -160.032348632812 70.6344840663086, -160.037841796875 70.6344840663086, -160.037841796875 70.6363054807905))\n",
    "\n",
    "# Solution\n",
    "all_data_ookla['geometry'] = gpd.GeoSeries.from_wkt(all_data_ookla['tile'])\n",
    "all_data_ookla = gpd.GeoDataFrame(all_data_ookla)\n",
    "all_data_ookla.crs = \"EPSG:4326\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the internet speed data on the school catchment\n",
    "all_data_ookla_joined = all_data_ookla.sjoin(df_schools)\n",
    "\n",
    "# Group by school and compute the average upload/download speeds (catchment_average_download_mbps, catchment_average_upload_mbps)\n",
    "all_data_ookla_joined = all_data_ookla_joined.groupby([\"school_id\", \"lat\", \"lon\", \"Shape_Leng\", \"Shape_Area\"]).agg({\"avg_d_kbps\": ['mean'], \"avg_u_kbps\": [\"mean\"], \"devices\": [\"sum\"]})\n",
    "all_data_ookla_joined = all_data_ookla_joined.droplevel(axis=1, level=1).reset_index()\n",
    "\n",
    "# Next step related to these fields is to save a new geojson file including these school catchment internet speeds\n",
    "\n",
    "# For the metrics: \"school_has_connectivity\", \"school_download_mbps\" and \"school_upload_mbps\". I am currently not sure how to calculate them.\n",
    "# This is my only idea: Use lat/lon cordinates (Point) of the schools + shape_leng/area to establish a polygon for each school\n",
    "# Then check if there are any internet speed tests inside these school polygons\n",
    "# If there are speed tests inside, then compute the average and set \"school_has_connectivity\" to True, otherwise False.\n",
    "\n",
    "school_points = gpd.GeoDataFrame(df_schools[[\"school_id\"]], geometry=gpd.points_from_xy(df_schools.lon, df_schools.lat))\n",
    "# ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### School catchment children\n",
    "\n",
    "In this part we are gonna use population images of Thailand, downloaded from The Humanitarian Data Exchange.\n",
    "I was looking for access to a REST API so that I can use the python `requests` library. But I couldnt see any such API documentation for this, only a Python SDK on humdata.org. I decided to just manually download the relevant files directly from the website: https://data.humdata.org/dataset/worldpop-age-and-sex-structures-for-thailand\n",
    "\n",
    "We wanted data for kids (male/female) aged 5-20 years old. Therefore I set out to download the relevant files such as:\n",
    "`tha_m_5_2020.tif`, `tha_f_5_2020.tif`, `tha_m_10_2020.tif`, `tha_10_5_2020.tif` etc\n",
    "\n",
    "I saved these in a data folder and started exploring the data and how to work with it (also completely new to me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore working with GEOTIF files and plot raster image\n",
    "import rasterio as rs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "raster_data = rs.open(\"data/tha_m_5_2020.tif\")\n",
    "raster_data_band = raster_data.read(1)\n",
    "raster_data_band[raster_data_band < 0] = None\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14, 14\n",
    "plt.imshow(np.log10(raster_data_band+1),)\n",
    "bar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "\n",
    "# Compute school population by masking the population map with each schools catchment area \n",
    "def get_school_population(school_polygon, raster_layer):\n",
    "    masked, out_transform = mask(raster_layer, [school_polygon])\n",
    "    pop_estimate = masked[0][masked[0] > 0].sum()\n",
    "    return pop_estimate\n",
    "\n",
    "# I only donwloaded 2 files, as processing them takes a lot of time this way.\n",
    "# Maybe the raster images can actually be added together (merged) in an efficient way, as they are basically x, y coordinates with\n",
    "# a value for each point.\n",
    "kids_population_rasters = [\n",
    "    \"data/tha_m_5_2020.tif\",\n",
    "    \"data/tha_m_10_2020.tif\",\n",
    "]\n",
    "\n",
    "for raster in kids_population_rasters:\n",
    "    raster_data = rs.open(raster)\n",
    "    if \"population\" not in df_schools:\n",
    "        df_schools['population'] = df_schools['geometry'].apply(get_school_population, raster_layer=raster_data)\n",
    "    else:\n",
    "        df_schools['population'] = df_schools['population'] + df_schools['geometry'].apply(get_school_population, raster_layer=raster_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the data to a new geojson file\n",
    "# Eventually, after completing all the steps above and doing that for all the datasets,\n",
    "# the final data can be saved to a new geojson file\n",
    "\n",
    "df_schools.to_file(\"schools_by_district_new.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "I completed a bit of everything, but didnt finish it all. The biggest challenges that took time for me were:\n",
    "1. I had to learn about geospatial data processing almost from scratch, I only briefly worked with these libraries before (Except for Dask and Pandas)\n",
    "2. Today I only had an old Macbook Air with 8GB of memory available, and processing of large files were very time consuming.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8197d3941375adf594f60828c576962656fcc0afeda96493efccd183f9e53b7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
